{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @Author: Rutvik Patel (17BCE0729)\n",
    "# @Date: 29 August 2020, 4 September 2020\n",
    "# @Description: Document Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from string import punctuation\n",
    "import math\n",
    "import requests \n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning data (removing HTML tags and other special characters, references, etc.)\n",
    "def remove_html_tags(text):\n",
    "    clean = re.compile('<.*?>')\n",
    "    return re.sub(clean, '', text)\n",
    "def remove_newline(text):\n",
    "    clean = re.compile('\\n')\n",
    "    return re.sub(clean, '', text)\n",
    "def remove_refs(text):\n",
    "    clean = re.compile('\\[.*\\]')\n",
    "    return re.sub(clean, '', text)\n",
    "\n",
    "def extractContentByTag(soup, TAG):\n",
    "    contents = []\n",
    "    if TAG == 'p':\n",
    "        contents = ''\n",
    "        for tag in soup.find('div', {'class:', 'storyWrap'}).findAll(TAG):\n",
    "            contents += (remove_refs(remove_newline(remove_html_tags(tag.getText()))))\n",
    "        contents = contents[ : contents.rfind('\\xa0To\\xa0subscribe\\xa0to\\xa0National Geographic Traveller India\\xa0and\\xa0National\\xa0Geographic')]\n",
    "        \n",
    "    else:\n",
    "        for parentTag in soup.findAll('div', {'class:', 'cDescription'}):\n",
    "            for tag in parentTag.findAll(TAG):\n",
    "                if 'href' in tag.attrs.keys():\n",
    "                    if not tag.attrs['href'].startswith('http', 0): #Filter useless URLs\n",
    "                        continue\n",
    "                    contents.append(tag.attrs['href'])\n",
    "            \n",
    "    return(contents)\n",
    "\n",
    "def extractURLs(seedURL):\n",
    "    req = requests.get(seedURL)\n",
    "    soup = BeautifulSoup(req.content, 'html5lib')\n",
    "    URLs = extractContentByTag(soup, 'a')\n",
    "    return(URLs)\n",
    "\n",
    "def extractArticles(URLs):\n",
    "    corpus = {}\n",
    "    reqs = [requests.get(URL) for URL in URLs]\n",
    "    soups = [BeautifulSoup(req.content, 'html5lib') for req in reqs] \n",
    "    corpus = {URLs[i] : extractContentByTag(soups[i], 'p') for i in range(len(URLs))}\n",
    "    return(corpus)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting data frtom the URL using beautiful soup\n",
    "URLs = extractURLs('http://www.natgeotraveller.in/author/lakshmi-sankaran/')\n",
    "corpus = extractArticles(URLs)\n",
    "df = pd.DataFrame([(URL, len(corpus[URL])) for URL in corpus], index = ['Article ' + str(i + 1) for i in range(len(corpus.keys()))], columns = ['Article Link', 'Article Length'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getStopWords():\n",
    "    StopWords = set(nltk.corpus.stopwords.words('english'))\n",
    "    StopWords.update(set(punctuation))\n",
    "    StopWords.update(set(['a','they','the','his','so','and','were','from','that','of','in','only','with','to']))\n",
    "    return(StopWords)\n",
    "    \n",
    "def bagOfWords(corpus, processedCorpus, processedCorpusKeys):\n",
    "    StopWords = getStopWords()\n",
    "    for article in corpus:\n",
    "        doc = nltk.tokenize.word_tokenize(corpus[article])\n",
    "        doc = [word.lower() for word in doc if not word.lower() in StopWords]\n",
    "        processedCorpus[article] = doc\n",
    "        processedCorpusKeys.append(article)\n",
    "    BOWR = {} #bag of words representation\n",
    "    terms = []\n",
    "    for article in processedCorpus:\n",
    "        terms.extend(processedCorpus[article])\n",
    "    terms = set(terms)\n",
    "    for term in terms:\n",
    "        row = []\n",
    "        for article in processedCorpusKeys:\n",
    "            count = 0\n",
    "            for t in processedCorpus[article]:\n",
    "                if t == term:\n",
    "                    count += 1\n",
    "            row.append(count)\n",
    "        BOWR[term] = row\n",
    "    BOWR['total_terms'] = [len(processedCorpus[article]) for article in processedCorpusKeys]\n",
    "    return(BOWR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processedCorpus = {}\n",
    "processedCorpusKeys = []\n",
    "bag = bagOfWords(corpus, processedCorpus, processedCorpusKeys)\n",
    "df = pd.DataFrame(bag, index = [URL for URL in URLs])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF matrix\n",
    "def getTFMatrix(bag, processedCorpus, processedCorpusKeys):\n",
    "    totals = [bag['total_terms'][i] for i in range(len(processedCorpusKeys))]\n",
    "    TFMatrix = {}\n",
    "    TFMatrix = {term : [bag[term][i] / totals[i] for i in range(len(processedCorpusKeys))] for term in bag.keys()}\n",
    "    del TFMatrix['total_terms']\n",
    "    return(TFMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TF = getTFMatrix(bag, processedCorpus, processedCorpusKeys)\n",
    "df = pd.DataFrame(TF, index = URLs)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getIDFVector(bag, processedCorpus, processedCorpusKeys):\n",
    "    IDF = {}\n",
    "    terms = []\n",
    "    for article in processedCorpusKeys:\n",
    "        terms.extend(processedCorpus[article])\n",
    "    terms = set(terms)\n",
    "    for term in terms:\n",
    "        appears = [0 for _ in range(len(processedCorpusKeys))]\n",
    "        for i in range(len(processedCorpusKeys)):\n",
    "            if term in processedCorpus[processedCorpusKeys[i]]:\n",
    "                appears[i] = 1\n",
    "        IDF[term] = 0 if sum(appears) == 0 else math.log((1 + len(processedCorpusKeys)) / sum(appears))\n",
    "    return(IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDV = getIDFVector(bag, processedCorpus, processedCorpusKeys)\n",
    "df = pd.DataFrame(IDV, index = ['IDF values'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTF_IDFMatrix(TF, IDV, processedCorpusKeys):\n",
    "    TF_IDF = {}\n",
    "    for term in TF:\n",
    "        TF_IDF[term] = [TF[term][i] * IDV[term] for i in range(len(processedCorpusKeys))]\n",
    "    return(TF_IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TF_IDF = getTF_IDFMatrix(TF, IDV, processedCorpusKeys)\n",
    "df = pd.DataFrame(TF_IDF, index = URLs)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(TF_IDF, processedCorpusKeys):\n",
    "    norm = {}\n",
    "    denos = [0 for _ in range(len(processedCorpusKeys))]\n",
    "    for i in range(len(processedCorpusKeys)):\n",
    "        denos[i] += sum([TF_IDF[term][i] ** 2 for term in TF_IDF])\n",
    "    for i in range(len(denos)):\n",
    "        denos[i] = denos[i] ** 0.5\n",
    "    for term in TF_IDF:\n",
    "        norm[term] = [TF_IDF[term][i] / denos[i] for i in range(len(processedCorpusKeys))]\n",
    "    return(norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm = normalize(TF_IDF, processedCorpusKeys)\n",
    "df = pd.DataFrame(norm, index = ['Article ' + str(i + 1) for i in range(len(processedCorpusKeys))])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cosine document similarity:\n",
    "def getDocumentCosineSimilarityResult(norm, processedCorpusKeys):\n",
    "    similarityRes = {}\n",
    "    for i in range(0, len(processedCorpusKeys)):\n",
    "        for j in range(i + 1, len(processedCorpusKeys)):\n",
    "            similarity = 0\n",
    "            for term in norm:\n",
    "                similarity += norm[term][i] * norm[term][j]\n",
    "            res = 'Cosine similarity of URL ' + str(i + 1) + ' with URL ' + str(j + 1) + ' is: '\n",
    "            similarityRes[res] = similarity\n",
    "    return(similarityRes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = getDocumentCosineSimilarityResult(norm, processedCorpusKeys)\n",
    "df = pd.DataFrame([result.keys(), result.values()], index = ['Article Pair', 'Cosine Similarity']).transpose()\n",
    "df.sort_values('Cosine Similarity', axis = 0, ascending = False, inplace = True, kind = 'quicksort')\n",
    "print('Key = higher the value of cosine similarity (angle - dot product), the more similar an article pair is')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Document euclidean distances:\n",
    "def getDocumentEuclideanDistanceResult(norm, processedCorpusKeys):\n",
    "    similarityRes = {}\n",
    "    for i in range(0, len(processedCorpusKeys)):\n",
    "        for j in range(i + 1, len(processedCorpusKeys)):\n",
    "            similarity = 0\n",
    "            for term in norm:\n",
    "                similarity += math.pow(norm[term][i] - norm[term][j], 2)\n",
    "            similarity = math.pow(similarity, 0.5)\n",
    "            res = 'Euclidean distance of URL ' + str(i + 1) + ' with URL ' + str(j + 1) + ' is: '\n",
    "            similarityRes[res] = similarity\n",
    "    return(similarityRes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = getDocumentEuclideanDistanceResult(norm, processedCorpusKeys)\n",
    "df = pd.DataFrame([result.keys(), result.values()], index = ['Document Pair', 'Euclidean Distance']).transpose()\n",
    "df.sort_values('Euclidean Distance', axis = 0, ascending = True, inplace = True, kind = 'quicksort')\n",
    "print('Key = lesser the value of Euclidean distance (geometric distance in n-dimentional Euclidean space), the more similar an article pair is')\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
